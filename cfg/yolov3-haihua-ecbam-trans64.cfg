[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=16
subdivisions=1
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 500200
policy=steps
steps=400000,450000
scales=.1,.1

# output_filters的索引比module_list中+1
# 因此module_list的index=mdef的index
# medf_index=0
[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

# TODO Downsample

# mdef_index=1
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

# mdef_index =2
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky

# mdef_index=3
[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

# mdef_index=4
# routs[0] = 1  # self.routs[i]是True或者False掩码，out会根据self.routs[i]为True或者false将module的输出 x append进out
# 这里routs[i]=n仅表示的是module_list中索引为第n个module的输出会被实际append进out
[shortcut]
from=-3
activation=linear

# TODO Downsample
# mdef_index=5
[convolutional]
batch_normalize=1
filters=128
size=3
stride=2
pad=1
activation=leaky

# mdef_index=6
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# mdef_index=7
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

# mdef_index=8
[shortcut]
from=-3
activation=linear

# mdef_index=9
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

# mdef_index=10
[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

# mdef_index=11
[shortcut]
from=-3
activation=linear
############################################################################# TODO 3
# TODO  Downsample
# mdef_index=12
[convolutional]
batch_normalize=1
filters=256
size=3
stride=2
pad=1
activation=leaky

# mdef_index=13
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=14
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=15
[shortcut]
from=-3
activation=linear

# mdef_index=16
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=17
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=18
[shortcut]
from=-3
activation=linear

# mdef_index=19
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=20
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=21
[shortcut]
from=-3
activation=linear

# mdef_index=22
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=23
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=24
[shortcut]
from=-3
activation=linear

# mdef_index=25
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=26
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=27
[shortcut]
from=-3
activation=linear

# mdef_index=28
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=29
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=30
[shortcut]
from=-3
activation=linear

# mdef_index=31
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=32
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=33
[shortcut]
from=-3
activation=linear

# mdef_index=34
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=35
[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

# mdef_index=36   原36
[shortcut]
from=-3
activation=linear

# TODO  Downsample
# TODO ######################################################################  4
# mdef_index=37
[convolutional]
batch_normalize=1
filters=512
size=3
stride=2
pad=1
activation=leaky

# mdef_index=38
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=39
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# mdef_index=40
[shortcut]
from=-3
activation=linear

# mdef_index=41
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=42
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# mdef_index=43
[shortcut]
from=-3
activation=linear

# mdef_index=44
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=45
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# mdef_index=46
[shortcut]
from=-3
activation=linear

# mdef_index=47
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=48
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# mdef_index=49
[shortcut]
from=-3
activation=linear

# mdef_index=50
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=51
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

# mdef_index=52
[shortcut]
from=-3
activation=linear

# mdef_index=53
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=54
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=None

# mdef_index=55
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=56
[shortcut]
from=-4
activation=linear

# mdef_index=57
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=58
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=None

# mdef_index=59
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=60
[shortcut]
from=-4
activation=linear

# mdef_index=61
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=62
[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=None

# mdef_index=63
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=64   # TODO FPN_2
[shortcut]
from=-3
activation=linear

# TODO Downsample  ######################################################5

# mdef_index=65
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=2
pad=1
activation=leaky

# mdef_index=66
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# mdef_index=67
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=None

# mdef_index=68
[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7

# mdef_index=69
[shortcut]
from=-4
activation=linear

# mdef_index=70
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# mdef_index=71
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=None

# mdef_index=72
[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7

# mdef_index=73
[shortcut]
from=-4
activation=linear

# mdef_index=74
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# mdef_index=75
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=None

# mdef_index=76
[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7

# mdef_index=77
[shortcut]
from=-4
activation=linear

# mdef_index=78
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# mdef_index=79
[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=None

# mdef_index=80 ######################################
[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7

# mdef_index=81
[shortcut]
from=-4
activation=linear

######################

# mdef_index=82
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky


[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=83
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7

# mdef_index=84
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

# mdef_index=85  # 同时这里也可以不需要再
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=86
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[ecbam]
eca_kernel_size = 7
sa_kernel_size = 7
# yolo-layer1检测头

# mdef_index=87
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=88
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

# mdef_index=89
[convolutional]
size=1
stride=1
pad=1
filters=1254
activation=linear


# mdef_index=90
[yolo]
mask = 6,7,8,9,10,11
anchors = 18,20,  39,35,  41,65,  61,50,  75,78,  107,56,  100,93,  69,136,  116,131,  158,101,  159,163,  222,210
classes=204
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1

# mdef_index=91
[route]
layers = -5

# mdef_index=92
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

# mdef_index=93
[upsample]
stride=2

# mdef_index=94
[route]
layers = -1, 64

# TODO 增加的ecbam将FPN送来的前面的卷积层的index增加了，后面训练的时候需要重新更新

# mdef_index=95
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=96
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=97
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky


# mdef_index=98
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=99
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky


[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# yolo-layer2检测头

# mdef_index=100
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7


# mdef_index=101
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=512
activation=leaky

# mdef_index=102
[convolutional]
size=1
stride=1
pad=1
filters=1254
activation=linear

# mdef_index=103
[yolo]
mask = 4,5,6,7,8,9
anchors = 18,20,  39,35,  41,65,  61,50,  75,78,  107,56,  100,93,  69,136,  116,131,  158,101,  159,163,  222,210
classes=204
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1


# mdef_index=104
[route]
layers = -5

# mdef_index=105 这里是不需要增加ecbam的，因为这里是将之前的特征图拿过来，并且对通道信息整合
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=106
[upsample]
stride=2

# mdef_index=107
[route]
layers = -1, 36


# mdef_index=108
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7


# mdef_index=109
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=110
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# mdef_index=111  当时，这里放ecbam的初衷是为了在检测头中也加入ecbam
[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=112
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# yolo-layer3检测头

# mdef_index=113
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[ecbam]
eca_kernel_size = 5
sa_kernel_size = 7

# mdef_index=114
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

# mdef_index=115
[convolutional]
size=1
stride=1
pad=1
filters=836
activation=linear

# mdef_index=116
[yolo]
mask = 0,1,2,3
anchors = 18,20,  39,35,  41,65,  61,50,  75,78,  107,56,  100,93,  69,136,  116,131,  158,101,  159,163,  222,210
classes=204
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1